{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's time for `classification` problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We spend time on solving regression problem!<br>\n",
    "For classification problem we need to make 2 things!\n",
    "\n",
    "- Softmax\n",
    "- Cross Entropy Loss\n",
    "\n",
    "**But before let's make `slice` function** to manage the tensor's more easily!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Slice`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetItem(Function):\n",
    "    def __init__(self, slices):\n",
    "        self.slices = slices\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x[self.slices]\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        x, = self.inputs\n",
    "        gx = GetItemGrad(self.slices, x.shape)(gy)\n",
    "        return gx\n",
    "    \n",
    "def get_item(x, slices):\n",
    "    return GetItem(slices)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no funciton for `Slice` so let's make another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetItemGrad(Funtion):\n",
    "    def __init__(self, slices, in_shape):\n",
    "        self.slices = slices\n",
    "        self.in_shape = in_shape\n",
    "        \n",
    "    def forward(self, gy):\n",
    "        gx = np.zeros(self.in_shape)\n",
    "        np.add.at(gx, self.slices, gy)\n",
    "        return gx\n",
    "    \n",
    "    def backward(self, ggx):\n",
    "        return get_item(ggx, self.slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How `np.add.at` works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.zeros((2, 3))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.ones((3,))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slices = 1\n",
    "np.add.at(a, slices, b)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same work :)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[slices] = b\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable([4 5 6])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "x = Variable(np.array([[1, 2, 3],\n",
    "                       [4, 5, 6]]))\n",
    "y = F.get_item(x, 1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable([[0. 0. 0.]\n",
      "          [1. 1. 1.]])\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable([[1 2 3]\n",
      "          [1 2 3]\n",
      "          [4 5 6]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1, 2, 3],\n",
    "                       [4, 5, 6]]))\n",
    "indices = np.array([0, 0, 1])\n",
    "y = F.get_item(x, indices)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable([[2. 2. 2.]\n",
      "          [1. 1. 1.]])\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does `slices` handle `1:3`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndiceCheck:\n",
    "    def __getitem__(self, slices):\n",
    "        print(slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic = IndiceCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "ic[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slice(1, 3, None)\n"
     ]
    }
   ],
   "source": [
    "ic[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "ic[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable([[4 5 6]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.get_item(x, slice(1, 3, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "slice(1, 3, None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice(1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Slice` for Variable `__getitem__`\n",
    "\n",
    "```python\n",
    "Variable.__getitem__ = F.get_item\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable([[2. 2. 2.]\n",
      "          [1. 1. 1.]])\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable([4 5 6])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable\n",
    "\n",
    "x = Variable(np.array([[1, 2, 3],\n",
    "                       [4, 5, 6]]))\n",
    "y = x[1]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable([[0. 0. 0.]\n",
      "          [1. 1. 1.]])\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable([3 6])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1, 2, 3],\n",
    "                       [4, 5, 6]]))\n",
    "y = x[:,2]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable([[0. 0. 1.]\n",
      "          [0. 0. 1.]])\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Softmax` Function\n",
    "\n",
    "## $p_{k} = \\frac{exp(y_{k})}{\\sum^{n}_{i=1} exp(y_{i})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero.models import MLP\n",
    "\n",
    "model = MLP((10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable([[0.44170413 0.46544621 0.30717265]])\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.2, -0.4]])\n",
    "y = model(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`yield from` used to make generator inside generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero import Variable, as_variable\n",
    "import dezero.functions as F\n",
    "\n",
    "def softmax1d(x):\n",
    "    x = as_variable(x)\n",
    "    y = F.exp(x)\n",
    "    sum_y = F.sum(y)\n",
    "    return y / sum_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable([[0.34504752 0.35333769 0.30161479]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[0.2, -0.4]]))\n",
    "y = model(x)\n",
    "p = softmax1d(y)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But what if with batch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable([[0.17299718 0.17715364 0.15122123]\n",
      "          [0.17739957 0.16770525 0.15352312]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[0.2, -0.4],\n",
    "                       [0.3, -0.5]]))\n",
    "y = model(x)\n",
    "p = softmax1d(y)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see it breaks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_simple(x, axis=1):\n",
    "    x = as_variable(x)\n",
    "    y = F.exp(x)\n",
    "    sum_y = F.sum(y, axis=axis, keepdims=True)\n",
    "    return y / sum_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable([[0.34504752 0.35333769 0.30161479]\n",
      "          [0.35577543 0.33633344 0.30789113]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[0.2, -0.4],\n",
    "                       [0.3, -0.5]]))\n",
    "y = model(x)\n",
    "p = softmax_simple(y)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works again :)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sidetalk! - `Max`, `Min`, `Clip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3]\n",
      " [5]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3],\n",
    "              [3, 5, 4]])\n",
    "y = x.max(axis=1, keepdims=True)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False,  True],\n",
       "       [False,  True, False]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond = (x == y)\n",
    "cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_backward_shape(x, axis):\n",
    "    if axis is None:\n",
    "        axis = range(x.ndim)\n",
    "    elif isinstance(axis, int):\n",
    "        axis = (axis,)\n",
    "    else:\n",
    "        axis = axis\n",
    "    \n",
    "    shape = [s if ax not in axis else 1 for ax, s in enumerate(x.shape)]\n",
    "    return shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Max(Function):\n",
    "    def __init__(self, axis=None, keepdims=False):\n",
    "        self.axis = axis\n",
    "        self.keepdims = keepdims\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x.max(axis=self.axis, keepdims=self.keepdims)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        x = self.inputs[0]\n",
    "        y = self.outputs[0]()\n",
    "        \n",
    "        shape = utils.max_backward_shape(x, self.axis)\n",
    "        gy = gy.reshape(shape)\n",
    "        y = y.reshape(shape)\n",
    "        \n",
    "        cond = (x.data == y.data)\n",
    "        gy = broadcast_to(gy, cond.shape)\n",
    "        \n",
    "        gx = gy * cond\n",
    "        \n",
    "        return gx\n",
    "    \n",
    "\n",
    "class Min(Max):\n",
    "    def forward(self, x):\n",
    "        y = x.min(axis=self.axis, keepdims=self.keepdims)\n",
    "        return y\n",
    "    \n",
    "def max(x, axis=None, keepdims=False):\n",
    "    return Max(axis, keepdims)(x)\n",
    "\n",
    "def min(x, axis=None, keepdims=False):\n",
    "    return Min(axis, keepdims)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [3 5 4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 3],\n",
       "       [3, 3, 3]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x)\n",
    "np.clip(x, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True,  True],\n",
       "       [ True, False, False]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (x >= 2) * (x <=3)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1],\n",
       "       [2, 0, 0]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [[1, 1, 1],\n",
    "     [2, 2, 2]]\n",
    "y * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clip(Function):\n",
    "    def __init__(self, x_min, x_max):\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = np.clip(x, self.x_min, self.x_max)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        x, = self.inputs\n",
    "        mask = (x.data >= self.x_min) * (x.data <= self.x_max)\n",
    "        gx = gy * mask\n",
    "        return gx\n",
    "    \n",
    "def clip(x, x_min, x_max):\n",
    "    return Clip(x_min, x_max)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again! general `Softmax`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $p_{k} = \\frac{exp(y_{k})}{\\sum^{n}_{i=1} exp(y_{i})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2 -0.4  0.4]\n",
      " [ 0.3 -0.5  0.7]]\n",
      "[[0.4]\n",
      " [0.7]]\n",
      "[[-0.2 -0.8  0. ]\n",
      " [-0.4 -1.2  0. ]]\n",
      "[[0.36098289 0.19811161 0.4409055 ]\n",
      " [0.34000264 0.15277303 0.50722433]]\n",
      "[[0.36098289 0.19811161 0.4409055 ]\n",
      " [0.34000264 0.15277303 0.50722433]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.2, -0.4, 0.4],\n",
    "              [0.3, -0.5, 0.7]])\n",
    "\n",
    "x_max = x.max(axis=1, keepdims=True)\n",
    "x_after_max = x - x_max\n",
    "print(x)\n",
    "print(x_max)\n",
    "print(x_after_max)\n",
    "\n",
    "y = np.exp(x_after_max)\n",
    "y_sum = y.sum(axis=1, keepdims=True)\n",
    "print(y / y_sum)\n",
    "\n",
    "y = np.exp(x)\n",
    "y_sum = y.sum(axis=1, keepdims=True)\n",
    "print(y / y_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `i`th input $y_{i}$ `j`'th output $p_{j}$\n",
    "\n",
    "### When `i` = `j`\n",
    "\n",
    "## $\\frac{ \\partial{p_{i}} }{ \\partial{y_{i}} } =\n",
    "\\frac{ \\partial{ \\frac{exp(y_{i})}{\\sum^{n}_{k=1} exp(y_{k})} } }\n",
    "     { \\partial{y_{i}} }$\n",
    "     \n",
    "---\n",
    "$y = \\frac{f(x)}{g(x)}\n",
    "= \\frac{\\partial{y}}{\\partial{x}}\n",
    "= \\frac{f'(x)g(x) - f(x)g'(x)}{g(x)^{2}}$\n",
    "---\n",
    "---\n",
    "\n",
    "## $\\frac{ \\partial{p_{i}} }{ \\partial{y_{i}} } =\n",
    "\\frac{ exp(y_{i})\\sum^{n}_{k=1} exp(y_{k})\\ -\\ exp(y_{i})exp(y_{i}) }\n",
    "     { (\\sum^{n}_{k=1} exp(y_{k}))^{2} }$\n",
    "## $\\quad\\; =\n",
    "\\frac{ exp(y_{i}) \\big[ \\sum^{n}_{k=1} exp(y_{k})\\ -\\ exp(y_{i}) \\big] }\n",
    "     { (\\sum^{n}_{k=1} exp(y_{k}))^{2} }$\n",
    "## $\\quad\\; =\n",
    "\\frac{ exp(y_{i}) }\n",
    "     { \\sum^{n}_{k=1} exp(y_{k}) }\n",
    "\\frac{ \\sum^{n}_{k=1} exp(y_{k})\\ -\\ exp(y_{i}) }\n",
    "     { \\sum^{n}_{k=1} exp(y_{k}) }\n",
    "$\n",
    "## $\\quad\\; =\n",
    "\\frac{ exp(y_{i}) }\n",
    "     { \\sum^{n}_{k=1} exp(y_{k}) }\n",
    "\\bigg(\n",
    "1 - \n",
    "\\frac{ exp(y_{i}) }\n",
    "     { \\sum^{n}_{k=1} exp(y_{k}) }\n",
    "\\bigg)\n",
    "$\n",
    "---\n",
    "\n",
    "## $\\therefore \\frac{ \\partial{p_{i}} }{ \\partial{y_{i}} } = p_{i}(1 - p_{i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `i`th input $y_{i}$ `j`'th output $p_{j}$\n",
    "\n",
    "### When `i` $\\neq$ `j`\n",
    "\n",
    "## $\\frac{ \\partial{p_{j}} }{ \\partial{y_{i}} } =\n",
    "\\frac{ \\partial{ \\frac{exp(y_{j})}{\\sum^{n}_{k=1} exp(y_{k})} } }\n",
    "     { \\partial{y_{i}} }$\n",
    "     \n",
    "---\n",
    "$y = \\frac{f(x)}{g(x)}\n",
    "= \\frac{\\partial{y}}{\\partial{x}}\n",
    "= \\frac{f'(x)g(x) - f(x)g'(x)}{g(x)^{2}}$\n",
    "---\n",
    "---\n",
    "\n",
    "## $\\frac{ \\partial{p_{i}} }{ \\partial{y_{i}} } =\n",
    "\\frac{ 0\\sum^{n}_{k=1} exp(y_{k})\\ -\\ exp(y_{j})exp(y_{i}) }\n",
    "     { (\\sum^{n}_{k=1} exp(y_{k}))^{2} }$\n",
    "## $\\quad\\; =\n",
    "\\frac{ - exp(y_{j})exp(y_{i}) }\n",
    "     { (\\sum^{n}_{k=1} exp(y_{k}))^{2} }$\n",
    "## $\\quad\\; =\n",
    "-\n",
    "\\frac{ exp(y_{j}) }\n",
    "     { \\sum^{n}_{k=1} exp(y_{k}) }\n",
    "\\frac{ exp(y_{i}) }\n",
    "     { \\sum^{n}_{k=1} exp(y_{k}) }\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "## $\\therefore \\frac{ \\partial{p_{j}} }{ \\partial{y_{i}} } = -p_{j}p_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\frac{ \\partial{p} }{ \\partial{y} }\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "p_{1}(1 - p_{1}) & -p_{2}p_{1}      & -p_{3}p_{1} \\\\\n",
    "-p_{1}p_{2}      & p_{2}(1 - p_{2}) & -p_{3}p_{2} \\\\\n",
    "-p_{1}p_{3}      & -p_{2}p_{3}      & p_{3}(1 - p_{3}) \n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "### when we look at gradient... for $y_{i}$\n",
    "\n",
    "### $p_{1}(1 - p_{1}) - p_{2}p_{1} - p_{3}p_{1}\n",
    "= p_{1}(1 - p_{1} - p_{2} - p_{3}) = p_{1}(1 - 1) = 0$\n",
    "\n",
    "### All gradient turns to zero :0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Function):\n",
    "    def __init__(self, axis=1):\n",
    "        self.axis = axis\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x - x.max(axis=self.axis, keepdims=True)\n",
    "        y = np.exp(y)\n",
    "        y /= y.sum(axis=self.axis, keepdims=True)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        y = self.outputs[0]()\n",
    "        gx = y * gy\n",
    "        sumdx = gx.sum(axis=self.axis, keepdims=True)\n",
    "        gx = gx - (y * sumdx)\n",
    "        return gx\n",
    "    \n",
    "def softmax(x, axis=1):\n",
    "    return Softmax(axis)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable([[0.64565631 0.35434369]\n",
      "          [0.68997448 0.31002552]])\n"
     ]
    }
   ],
   "source": [
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "x = Variable(np.array([[0.2, -0.4],\n",
    "                       [0.3, -0.5]]))\n",
    "p = F.softmax(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable([[1.11022302e-16 5.55111512e-17]\n",
       "          [0.00000000e+00 0.00000000e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the grad is **0** :0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Cross Entropy Loss` Function\n",
    "\n",
    "### $L = - {\\sum\\limits^{n}_{k=1} t_{k}log p_{k}}$\n",
    "\n",
    "when $t = (0, 0, 1)$ and $p = (p_{0}, p_{1}, p_{2})$\n",
    "\n",
    "### $L = -log p_{2}$\n",
    "\n",
    "---\n",
    "\n",
    "So we can say...\n",
    "\n",
    "### $L = -log p[t]$\n",
    "- Slicing!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero import Variable, as_variable\n",
    "import dezero.functions as F\n",
    "\n",
    "def softmax_cross_entropy_simple(x, t):\n",
    "    x, t = as_variable(x), as_variable(t)\n",
    "    N = x.shape[0]\n",
    "    \n",
    "    p = F.softmax(x)\n",
    "    p = F.clip(p, 1e-15, 1.0)\n",
    "    log_p = F.log(p)\n",
    "    tlog_p = log_p[np.arange(N), t.data]\n",
    "    y = -1 * F.sum(tlog_p) / N\n",
    "    return y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable(0.8965588080868547)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0.2, 0.3, 0.5],\n",
    "              [0.6, 0.1, 0.3]])\n",
    "t = np.array([2, 0])\n",
    "softmax_cross_entropy_simple(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable(0.008595870610124774)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(np.array([[0.2, 0.3, 5],\n",
    "                       [12, 0.1, 0.3]]))\n",
    "t = Variable(np.array([2, 0]))\n",
    "loss = softmax_cross_entropy_simple(x, t)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable([[ 4.04479731e-03  4.47019236e-03 -8.51498967e-03]\n",
       "          [-7.54199822e-06  3.39515119e-06  4.14684703e-06]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the gradient is back with `Cross Entropy`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Softmax Cross Entropy Loss`!\n",
    "\n",
    "---\n",
    "\n",
    "### At `Softmax`\n",
    "\n",
    "### `i` = `j`\n",
    "\n",
    "## $\\frac{ \\partial{p_{i}} }{ \\partial{y_{i}} } = p_{i}(1 - p_{i})$\n",
    "\n",
    "### `i` $\\neq$ `j`\n",
    "\n",
    "## $\\frac{ \\partial{p_{j}} }{ \\partial{y_{i}} } = -p_{j}p_{i}$\n",
    "\n",
    "---\n",
    "\n",
    "### At `Cross Entropy`\n",
    "\n",
    "### $L = - {\\sum\\limits^{n}_{k=1} t_{k}log p_{k}}$\n",
    "\n",
    "## $\\frac{ \\partial{L} }{ \\partial{y_{i}} } =\n",
    "\\frac{ \\partial{ \\big( - \\sum^{n}_{k=1} t_{k}log p_{k} } \\big) }\n",
    "     { \\partial{y_{i}} }$\n",
    "     \n",
    "## $\\quad\\; =\n",
    "-\n",
    "\\sum^{n}_{k=1} t_{k} \\frac{\\partial{log p_{k}}}{\\partial{y_{i}}}\n",
    "$\n",
    "\n",
    "## $\\quad\\; =\n",
    "-\n",
    "\\sum^{n}_{k=1} t_{k} \\frac{1}{p_{k}} \\frac{\\partial{p_{k}}}{\\partial{y_{i}}}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Softmax Gradient $\\frac{\\partial{log p_{k}}}{\\partial{y_{i}}}$ works different when $i = j$ and $i \\neq j$\n",
    "\n",
    "### $- \\sum^{n}_{k=1} t_{k} \\frac{1}{p_{k}} \\frac{\\partial{p_{k}}}{\\partial{y_{i}}}\n",
    "= - \\frac{t_{i}}{p_{i}} p_{i}(1 - p_{i})\n",
    "- \\sum^{n}_{k \\neq i} \\frac{t_{k}}{p_{k}} (-p_{k}p_{i})$\n",
    "\n",
    "### $\\qquad\\qquad\\qquad = \n",
    "- t_{i} + t_{i}p_{i} + \\sum^{n}_{k \\neq i} t_{k}p_{i}\n",
    "$\n",
    "\n",
    "### $\\qquad\\qquad\\qquad = \n",
    "- t_{i} + t_{i}p_{i} - t_{i}p_{i} + t_{i}p_{i} + \\sum^{n}_{k \\neq i} t_{k}p_{i}\n",
    "$\n",
    "\n",
    "### $\\qquad\\qquad\\qquad = - t_{i} + \\sum^{n}_{k=1} t_{k}p_{i}$\n",
    "### $\\qquad\\qquad\\qquad = - t_{i} + p_{i}\\sum^{n}_{k=1} t_{k}$\n",
    "### $\\qquad\\qquad\\qquad = - t_{i} + p_{i}$\n",
    "### $\\qquad\\qquad\\qquad = p_{i} - t_{i}$\n",
    "\n",
    "---\n",
    "\n",
    "We can see the Gradient is pretty simple :)\n",
    "\n",
    "### It's just $p - t$ = softmax(x) $ -\\ t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- `softmax`\n",
    "\n",
    "### $p_{k} = \\frac{exp(y_{k})}{\\sum^{n}_{i=1} exp(y_{i})}$\n",
    "\n",
    "- `log softmax`\n",
    "\n",
    "### $log(p_{k}) = log(exp(y_{k})) - log(\\sum^{n}_{i=1} exp(y_{i}))$\n",
    "### $ \\qquad\\;\\ = y_{k} - log(\\sum^{n}_{i=1} exp(y_{i}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x, axis=1):\n",
    "    x_max = x.max(axis=axis, keepdims=True)\n",
    "    y = x - x_max\n",
    "\n",
    "    y = np.exp(y)\n",
    "    y_sum = y.sum(axis=axis, keepdims=True)\n",
    "    \n",
    "    log_y_sum = np.log(y_sum)\n",
    "    log_z = x_max + log_y_sum\n",
    "    \n",
    "    log_p = x - log_z\n",
    "\n",
    "    return log_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ravel helps to make all the elements inside to be `1d array`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([[2], [0], [1], [0]])\n",
    "t.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $L = - {\\sum\\limits^{n}_{k=1} t_{k}log p_{k}}$\n",
    "\n",
    "## $\\frac{ \\partial{L} }{ \\partial{y_{i}} } = p_{i} - t_{i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(3)[t.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropy(Function):\n",
    "    def forward(self, x, t):\n",
    "        N = x.shape[0]\n",
    "        log_p = log_softmax(x)\n",
    "        log_p = log_p[np.arange(N), t.ravel()]\n",
    "        y = -log_p.sum() / np.float32(N)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        x, t = self.inputs\n",
    "        N, CLS_NUM = x.shape\n",
    "        \n",
    "        gy = gy / N\n",
    "        \n",
    "        # p_i\n",
    "        y = softmax(x)\n",
    "        \n",
    "        # t_i\n",
    "        t_onehot = np.eye(CLS_NUM, dtype=t.dtype)[t.data]\n",
    "        \n",
    "        # p_i - t_i\n",
    "        gx = (y - t_onehot) * gy\n",
    "        \n",
    "        return gx\n",
    "    \n",
    "def softmax_cross_entropy(x, t):\n",
    "    return SoftmaxCrossEntropy()(x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable(1.2923647287521973)\n",
      "Variable(0.8111358663141301)\n",
      "Variable(0.40483469446083364)\n",
      "Variable(0.2193841379002533)\n",
      "Variable(0.12876382875701547)\n",
      "Variable(0.08055995411287575)\n",
      "Variable(0.05366404403450009)\n",
      "Variable(0.03777797489187806)\n",
      "Variable(0.027826497488328217)\n",
      "Variable(0.021253890618229243)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from dezero import Variable\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "\n",
    "lr = 0.01\n",
    "\n",
    "model = MLP((10, 3))\n",
    "optimizer = optimizers.Adam(lr).setup(model)\n",
    "\n",
    "x = Variable(np.array([[0.2, -0.4],\n",
    "                       [0.3, 0.5],\n",
    "                       [1.3, -3.2],\n",
    "                       [2.1, 0.3]]))\n",
    "y = Variable(np.array([2, 0, 1, 0]))\n",
    "\n",
    "iters = 1000\n",
    "\n",
    "for i in range(iters):\n",
    "    y_pred = model(x)\n",
    "    loss = F.softmax_cross_entropy(y_pred, y)\n",
    "    \n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "    optimizer.update()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable([[ 0.0859358  -0.16529972  4.09196037]\n",
       "          [ 3.98952045 -2.77720881  0.13626685]\n",
       "          [-3.38982648  4.90290326  0.46156244]\n",
       "          [ 4.97053061 -2.04978722 -3.63391973]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
